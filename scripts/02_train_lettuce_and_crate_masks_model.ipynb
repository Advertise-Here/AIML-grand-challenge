{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e63791",
   "metadata": {},
   "source": [
    "---\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd49528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV version:  4.12.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from rich import print as rprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "# Filter the specific UserWarning from torch regarding TF32/matmul precision\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch\")\n",
    "\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('high')\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import albumentations as A\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import copy\n",
    "\n",
    "print('OpenCV version: ', cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4faf393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "seed_everything(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc0d23f",
   "metadata": {},
   "source": [
    "---\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7521a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Training Data CSV\n",
    "df_train = pd.read_csv('../data/raw/Training/Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7a6f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop image 332 due to corrupted file\n",
    "df_train = df_train[df_train['image_id'] != 332].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42bd23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 230 training images.\n"
     ]
    }
   ],
   "source": [
    "# Read Cropped RGB images - image values [0,255]\n",
    "\n",
    "bgr_path_train = '../data/processed/crops/Training/RGBImages/'\n",
    "\n",
    "cropped_rgb_images_train = {}\n",
    "\n",
    "for image_id in df_train['image_id'].values:\n",
    "    if image_id % 332 == 0:\n",
    "        continue\n",
    "    img = cv2.imread(bgr_path_train + 'cropped_RGB_' + str(image_id) + '.png', cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    cropped_rgb_images_train[image_id] = img\n",
    "print(f'Loaded {len(cropped_rgb_images_train)} training images.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18546d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 230 training depth images.\n"
     ]
    }
   ],
   "source": [
    "# Read Depth images - image values - 16 bit\n",
    "depth_path_train = '../data/processed/crops/Training/DepthImages/'\n",
    "\n",
    "cropped_depth_images_train = {}\n",
    "\n",
    "for image_id in df_train['image_id'].values:\n",
    "    if image_id % 332 == 0:\n",
    "        continue\n",
    "    img = cv2.imread(depth_path_train + 'cropped_Depth_' + str(image_id) + '.png', cv2.IMREAD_UNCHANGED)\n",
    "    cropped_depth_images_train[image_id] = img\n",
    "print(f'Loaded {len(cropped_depth_images_train)} training depth images.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6af5e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 800, 3), (800, 800))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cropped_rgb_images_train[1].shape, cropped_depth_images_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d27d1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specific_mask(mask_uint8, target_name):\n",
    "    \"\"\"\n",
    "    Returns a binary boolean mask for a specific object class.\n",
    "    \n",
    "    INPUTS:\n",
    "        mask_uint8 (np.ndarray): The 800x800 grayscale image from CVAT \n",
    "                                 (values 0, 145, 169).\n",
    "        target_name (str):       The object to isolate: 'lettuce' or 'crate'.\n",
    "        \n",
    "    OUTPUT:\n",
    "        binary_mask (np.ndarray): A boolean array where True represents \n",
    "                                  the requested object.\n",
    "    \"\"\"\n",
    "    if target_name.lower() == 'lettuce':\n",
    "        return mask_uint8 == 169\n",
    "    elif target_name.lower() == 'crate':\n",
    "        return mask_uint8 == 145\n",
    "    else:\n",
    "        raise ValueError(\"target_name must be 'lettuce' or 'crate'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad2c93a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to train: 70 Lettuce masks and 70 Crate masks.\n"
     ]
    }
   ],
   "source": [
    "# Load Segmentation Masks - image values [0,255]\n",
    "# Read file names from os as not all images have masks. File names are in format 'cropped_RGB_{image_id}.png'\n",
    "\n",
    "mask_path_train = '../data/labels/SegmentationClass/'\n",
    "\n",
    "lettuce_masks_800 = {}\n",
    "crate_masks_800 = {}\n",
    "\n",
    "for filename in os.listdir(mask_path_train):\n",
    "    if filename.endswith('.png'):\n",
    "        image_id = int(filename.split('_')[2].split('.')[0])\n",
    "        raw_img = cv2.imread(os.path.join(mask_path_train, filename), cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Specialist 1: Lettuce (Intensity 169)\n",
    "        lettuce_masks_800[image_id] = (raw_img == 169).astype(np.uint8)\n",
    "        \n",
    "        # Specialist 2: Crate (Intensity 145)\n",
    "        crate_masks_800[image_id] = (raw_img == 145).astype(np.uint8)\n",
    "\n",
    "print(f\"Ready to train: {len(lettuce_masks_800)} Lettuce masks and {len(crate_masks_800)} Crate masks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65c351f",
   "metadata": {},
   "source": [
    "---\n",
    "### Train segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9596a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The \"Infinite Variety\" Augmentation Pipeline\n",
    "# This makes 70 images look like 7,000 to the model\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    # Affine replaces ShiftScaleRotate\n",
    "    A.Affine(\n",
    "        translate_percent={\"x\": (-0.06, 0.06), \"y\": (-0.06, 0.06)},\n",
    "        scale=(0.9, 1.1),\n",
    "        rotate=(-45, 45),\n",
    "        p=0.5\n",
    "    ),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "])\n",
    "\n",
    "class CompetitiveDataset(Dataset):\n",
    "    def __init__(self, rgb_dict, mask_dict, ids, size=(512, 512), transform=None):\n",
    "        self.rgb_dict = rgb_dict\n",
    "        self.mask_dict = mask_dict\n",
    "        self.ids = ids\n",
    "        self.size = size\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.ids[idx]\n",
    "        img = cv2.resize(self.rgb_dict[key], self.size, interpolation=cv2.INTER_AREA)\n",
    "        mask = cv2.resize(self.mask_dict[key].astype(np.uint8), self.size, interpolation=cv2.INTER_NEAREST)\n",
    "        mask = (mask > 0).astype(np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img, mask=mask)\n",
    "            img, mask = augmented['image'], augmented['mask']\n",
    "\n",
    "        img_t = torch.from_numpy(img.astype(np.float32) / 255.0).permute(2, 0, 1)\n",
    "        mask_t = torch.from_numpy(mask).unsqueeze(0)\n",
    "        return img_t, mask_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5e12737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_specialist(model, train_loader, val_loader, optimizer, scheduler, criterion, epochs=100, name=\"Specialist\"):\n",
    "    # Ensure the weights directory exists\n",
    "    os.makedirs('../weights', exist_ok=True)\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    best_iou = 0.0  # \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # --- TRAINING PHASE ---\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"{name} Train - Epoch {epoch+1}/{epochs}\")\n",
    "        for imgs, msks in pbar:\n",
    "            imgs, msks = imgs.cuda(), msks.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, msks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # --- VALIDATION PHASE ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total_iou = 0.0\n",
    "        with torch.no_grad():\n",
    "            for imgs, msks in val_loader:\n",
    "                imgs, msks = imgs.cuda(), msks.cuda()\n",
    "                outputs = model(imgs)\n",
    "                \n",
    "                # Calculate Loss\n",
    "                loss = criterion(outputs, msks)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate IoU (Binary)\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                intersection = (preds * msks).sum()\n",
    "                union = (preds + msks).gt(0).sum()\n",
    "                iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "                total_iou += iou.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_iou = total_iou / len(val_loader)\n",
    "        \n",
    "        # Step scheduler based on Validation Loss\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Check if this is the best spatial fit (IoU) so far\n",
    "        if avg_iou > best_iou:\n",
    "            best_iou = avg_iou\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            save_path = f\"../weights/best_{name.lower()}_segmentation_model.pth\"\n",
    "            torch.save(best_model_wts, save_path)\n",
    "            print(f\"--> [NEW BEST IoU] {name}: {avg_iou:.4f} (Saved to {save_path})\")\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"{name} Ep {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | IoU: {avg_iou:.4f} | LR: {current_lr:.6f}\")\n",
    "\n",
    "    # Load the best weights back before returning to the main notebook\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4138cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Loss: Dice (shape) + BCE (pixel-level detail)\n",
    "# Use BCEWithLogitsLoss because smp.Unet does NOT have sigmoid by default\n",
    "dice_loss = smp.losses.DiceLoss(mode='binary')\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def criterion(outputs, masks):\n",
    "    return dice_loss(torch.sigmoid(outputs), masks) + bce_loss(outputs, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feac404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CREATE TRAIN/VAL SPLIT ---\n",
    "all_ids = list(lettuce_masks_800.keys())\n",
    "train_ids, val_ids = train_test_split(all_ids, test_size=0.15, random_state=42)\n",
    "\n",
    "# BATCH\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 4 \n",
    "\n",
    "# --- SET UP LETTUCE SPECIALIST ---\n",
    "lettuce_model = smp.Unet(encoder_name=\"efficientnet-b3\", encoder_weights=\"imagenet\", in_channels=3, classes=1).cuda()\n",
    "lettuce_opt = torch.optim.AdamW(lettuce_model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "lettuce_sched = ReduceLROnPlateau(lettuce_opt, mode='min', factor=0.5, patience=10)\n",
    "l_train_ds = CompetitiveDataset(cropped_rgb_images_train, lettuce_masks_800, train_ids, size=(512,512), transform=train_transform)\n",
    "l_val_ds = CompetitiveDataset(cropped_rgb_images_train, lettuce_masks_800, val_ids, size=(512,512), transform=None)\n",
    "l_train_loader = DataLoader(l_train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=2)\n",
    "l_val_loader = DataLoader(l_val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# --- SET UP CRATE SPECIALIST ---\n",
    "crate_model = smp.Unet(encoder_name=\"efficientnet-b3\", encoder_weights=\"imagenet\", in_channels=3, classes=1).cuda()\n",
    "crate_opt = torch.optim.AdamW(crate_model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "crate_sched = ReduceLROnPlateau(crate_opt, mode='min', factor=0.5, patience=10)\n",
    "c_train_ds = CompetitiveDataset(cropped_rgb_images_train, crate_masks_800, train_ids, size=(512,512), transform=train_transform)\n",
    "c_val_ds = CompetitiveDataset(cropped_rgb_images_train, crate_masks_800, val_ids, size=(512,512), transform=None)\n",
    "c_train_loader = DataLoader(c_train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=2)\n",
    "c_val_loader = DataLoader(c_val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# --- TRAIN BOTH ---\n",
    "# Train Lettuce (100 epochs for organic complexity)\n",
    "train_specialist(lettuce_model, l_train_loader, l_val_loader, lettuce_opt, lettuce_sched, criterion, epochs=100, name=\"Lettuce\")\n",
    "\n",
    "# Train Crate (50 epochs for geometric simplicity)\n",
    "train_specialist(crate_model, c_train_loader, c_val_loader, crate_opt, crate_sched, criterion, epochs=50, name=\"Crate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
