{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e63791",
   "metadata": {},
   "source": [
    "---\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd49528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV version:  4.12.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from rich import print as rprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "# Filter the specific UserWarning from torch regarding TF32/matmul precision\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch\")\n",
    "\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('high')\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import albumentations as A\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import copy\n",
    "import os\n",
    "import math\n",
    "\n",
    "print('OpenCV version: ', cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4faf393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "seed_everything(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc0d23f",
   "metadata": {},
   "source": [
    "---\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7521a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Training Data CSV\n",
    "df_train = pd.read_csv('../data/raw/Training/Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a6f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop image 332 due to corrupted file\n",
    "df_train = df_train[df_train['image_id'] != 332].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b42bd23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 230 training images.\n"
     ]
    }
   ],
   "source": [
    "# Read Cropped RGB images - image values [0,255]\n",
    "\n",
    "bgr_path_train = '../data/processed/crops/Training/RGBImages/'\n",
    "\n",
    "cropped_rgb_images_train = {}\n",
    "\n",
    "for image_id in df_train['image_id'].values:\n",
    "    if image_id % 332 == 0:\n",
    "        continue\n",
    "    img = cv2.imread(bgr_path_train + 'cropped_RGB_' + str(image_id) + '.png', cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    cropped_rgb_images_train[image_id] = img\n",
    "print(f'Loaded {len(cropped_rgb_images_train)} training images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18546d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 230 training depth images.\n"
     ]
    }
   ],
   "source": [
    "# Read Depth images - image values - 16 bit\n",
    "depth_path_train = '../data/processed/crops/Training/DepthImages/'\n",
    "\n",
    "cropped_depth_images_train = {}\n",
    "\n",
    "for image_id in df_train['image_id'].values:\n",
    "    if image_id % 332 == 0:\n",
    "        continue\n",
    "    img = cv2.imread(depth_path_train + 'cropped_Depth_' + str(image_id) + '.png', cv2.IMREAD_UNCHANGED)\n",
    "    cropped_depth_images_train[image_id] = img\n",
    "print(f'Loaded {len(cropped_depth_images_train)} training depth images.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65c351f",
   "metadata": {},
   "source": [
    "---\n",
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c8fc624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    model = smp.Unet(encoder_name=\"efficientnet-b3\", in_channels=3, classes=1).cuda()\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "lettuce_model = load_model(\"../weights/best_lettuce_segmentation_model.pth\")\n",
    "crate_model = load_model(\"../weights/best_crate_segmentation_model.pth\")\n",
    "\n",
    "def predict_biomass(rgb_800, depth_800):\n",
    "    # Preprocess for 512p model\n",
    "    img_512 = cv2.resize(rgb_800, (512, 512), interpolation=cv2.INTER_AREA)\n",
    "    img_t = torch.from_numpy(img_512.astype(np.float32)/255.0).permute(2,0,1).unsqueeze(0).cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        l_pred = torch.sigmoid(lettuce_model(img_t))\n",
    "        c_pred = torch.sigmoid(crate_model(img_t))\n",
    "        \n",
    "    # SCALE BACK TO 800p (Precision Handshake)\n",
    "    # Convert from boolean to uint8 so OpenCV can resize it\n",
    "    l_mask_bin = (l_pred > 0.5).cpu().numpy()[0,0].astype(np.uint8)\n",
    "    c_mask_bin = (c_pred > 0.5).cpu().numpy()[0,0].astype(np.uint8)\n",
    "    \n",
    "    # Now resize the uint8 masks\n",
    "    l_mask = cv2.resize(l_mask_bin, (800, 800), interpolation=cv2.INTER_NEAREST)\n",
    "    c_mask = cv2.resize(c_mask_bin, (800, 800), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    # CALCULATE Z-BASELINE (The Floor Fix)\n",
    "    # Using the median of the crate area to avoid outliers\n",
    "    z_floor = np.median(depth_800[c_mask == 1])\n",
    "    \n",
    "    # Isolate lettuce heights\n",
    "    lettuce_depths = depth_800[l_mask == 1]\n",
    "    heights = z_floor - lettuce_depths\n",
    "    \n",
    "    # Filter noise (e.g., negative heights)\n",
    "    heights = heights[heights > 0]\n",
    "    \n",
    "    # Calculate Volume/Biomass Proxy (Sum of heights * pixel area)\n",
    "    # Adjust this based on your regression model\n",
    "    biomass_estimate = np.sum(heights) \n",
    "    \n",
    "    return biomass_estimate, l_mask, c_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41bf9e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Biomass: 100%|██████████| 230/230 [00:20<00:00, 11.29it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_dataset(rgb_dict, depth_dict):\n",
    "    results = {}\n",
    "    for key in tqdm(rgb_dict.keys(), desc=\"Processing Biomass\"):\n",
    "        rgb = rgb_dict[key]\n",
    "        depth = depth_dict[key]\n",
    "        \n",
    "        # Calculate the 3D volume proxy using specialists\n",
    "        volume, _, _ = predict_biomass(rgb, depth)\n",
    "        results[key] = volume\n",
    "    return results\n",
    "\n",
    "# Process all splits\n",
    "train_volumes = process_dataset(cropped_rgb_images_train, cropped_depth_images_train) # volume estimate ultimately does not get used, just the masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d35e9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Train masks: 100%|██████████| 230/230 [00:25<00:00,  8.94it/s]\n"
     ]
    }
   ],
   "source": [
    "def save_all_masks(rgb_dict, depth_dict, split_name):\n",
    "    # Create directory for this split\n",
    "    output_dir = f\"../data/processed/masks_inferred/{split_name}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for key in tqdm(rgb_dict.keys(), desc=f\"Saving {split_name} masks\"):\n",
    "        # Re-run inference to get the high-res 800p masks\n",
    "        _, l_mask, c_mask = predict_biomass(rgb_dict[key], depth_dict[key])\n",
    "        \n",
    "        # Save Lettuce mask (0 or 255 for standard image viewing)\n",
    "        l_save_path = os.path.join(output_dir, f\"{key}_lettuce.png\")\n",
    "        cv2.imwrite(l_save_path, l_mask * 255)\n",
    "        \n",
    "        # Save Crate mask\n",
    "        c_save_path = os.path.join(output_dir, f\"{key}_crate.png\")\n",
    "        cv2.imwrite(c_save_path, c_mask * 255)\n",
    "\n",
    "# Run for all sets\n",
    "save_all_masks(cropped_rgb_images_train, cropped_depth_images_train, \"Train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
